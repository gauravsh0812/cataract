{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial implimentation-for backup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Import the UNet++ model\n",
    "from unetplus import NestedUNet\n",
    "\n",
    "class CataractDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_transform=None, mask_transform=None, augment=False):\n",
    "        self.data = []\n",
    "        self.questions = set()\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.data.append(row)\n",
    "                self.questions.add(row['Questions'])\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['Image_Paths']).convert('RGB')\n",
    "        mask = Image.open(item['Mask_Paths']).convert('L')\n",
    "        question = item['Questions']\n",
    "        label = item['Labels']\n",
    "\n",
    "        if self.augment:\n",
    "            image, mask = self.apply_augmentation(image, mask)\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, question, label\n",
    "\n",
    "    def apply_augmentation(self, image, mask):\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        angle = random.uniform(-10, 10)\n",
    "        image = TF.rotate(image, angle)\n",
    "        mask = TF.rotate(mask, angle)\n",
    "        brightness_factor = random.uniform(0.8, 1.2)\n",
    "        image = TF.adjust_brightness(image, brightness_factor)\n",
    "        return image, mask\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_channels=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        self.unetpp = NestedUNet(num_classes=num_classes, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unetpp(x)\n",
    "\n",
    "class LLMPromptEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 1024)\n",
    "\n",
    "    def forward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upconv_block(in_channels, 256),\n",
    "            self.upconv_block(256, 128),\n",
    "            self.upconv_block(128, 64),\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class LLMSupervisedSAM(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_channels=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(num_classes=num_classes, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.prompt_encoder = LLMPromptEncoder()\n",
    "        self.mask_decoder = MaskDecoder(in_channels=num_classes + 1024)\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "    def forward(self, image, questions):\n",
    "        image_features = self.image_encoder(image)\n",
    "        \n",
    "        batch_size = image.size(0)\n",
    "        prompt_features = self.prompt_encoder(questions)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            outputs = []\n",
    "            for feature in image_features:\n",
    "                resized_prompt = F.interpolate(prompt_features.unsqueeze(2).unsqueeze(3), \n",
    "                                               size=feature.shape[2:], \n",
    "                                               mode='bilinear', \n",
    "                                               align_corners=False)\n",
    "                combined_features = torch.cat([feature, resized_prompt], dim=1)\n",
    "                mask = self.mask_decoder(combined_features)\n",
    "                outputs.append(mask)\n",
    "            return outputs\n",
    "        else:\n",
    "            prompt_features = prompt_features.view(batch_size, -1, 1, 1).expand(-1, -1, image_features.shape[2], image_features.shape[3])\n",
    "            combined_features = torch.cat([image_features, prompt_features], dim=1)\n",
    "            mask = self.mask_decoder(combined_features)\n",
    "            return mask\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "def visualize_results(image, mask, prediction, question, output_path):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0))  # Change from (C, H, W) to (H, W, C)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.imshow(mask.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.imshow(prediction.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Question: {question}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device, output_folder):\n",
    "    train_folder = os.path.join(output_folder, \"train\")\n",
    "    val_folder = os.path.join(output_folder, \"validation\")\n",
    "    test_folder = os.path.join(output_folder, \"test\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5, verbose=True)\n",
    "    best_val_dice = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_dice = 0\n",
    "        for i, (images, masks, questions, _) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            #print(f\"Batch {i} - Image shape: {images.shape}, Mask shape: {masks.shape}\")\n",
    "            \n",
    "            # Ensure questions match the batch size\n",
    "            questions = questions[:batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions)\n",
    "            \n",
    "            if isinstance(outputs, list):  # deep supervision\n",
    "                loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                dice = sum([dice_coefficient(output, masks) for output in outputs]) / len(outputs)\n",
    "                output_for_vis = outputs[-1]  # Use the last output for visualization\n",
    "            else:\n",
    "                loss = criterion(outputs, masks)\n",
    "                dice = dice_coefficient(outputs, masks)\n",
    "                output_for_vis = outputs\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                visualize_results(\n",
    "                    images[0].cpu(),  # Single image\n",
    "                    masks[0].cpu(),   # Single mask\n",
    "                    output_for_vis[0].detach().cpu(),  # Single prediction\n",
    "                    questions[0], \n",
    "                    os.path.join(train_folder, f\"epoch_{epoch+1}_batch_{i}.png\")\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_dice = total_dice / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_dice = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, masks, questions, _) in enumerate(val_loader):\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                questions = questions[:batch_size]\n",
    "                outputs = model(images, questions)\n",
    "                \n",
    "                if isinstance(outputs, list):\n",
    "                    val_loss += sum([criterion(output, masks).item() for output in outputs]) / len(outputs)\n",
    "                    val_dice += sum([dice_coefficient(output, masks).item() for output in outputs]) / len(outputs)\n",
    "                    output_for_vis = outputs[-1]\n",
    "                else:\n",
    "                    val_loss += criterion(outputs, masks).item()\n",
    "                    val_dice += dice_coefficient(outputs, masks).item()\n",
    "                    output_for_vis = outputs\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    visualize_results(\n",
    "                        images[0].cpu(),\n",
    "                        masks[0].cpu(),\n",
    "                        output_for_vis[0].detach().cpu(),\n",
    "                        questions[0],\n",
    "                        os.path.join(val_folder, f\"epoch_{epoch+1}_batch_{i}.png\")\n",
    "                    )\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = val_dice / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Dice: {avg_val_dice:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_dice)\n",
    "\n",
    "        if avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            torch.save(model.state_dict(), os.path.join(output_folder, \"best_model.pth\"))\n",
    "\n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(torch.load(os.path.join(output_folder, \"best_model.pth\")))\n",
    "\n",
    "    # Test set evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, questions, _) in enumerate(test_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            questions = questions[:batch_size]\n",
    "            outputs = model(images, questions)\n",
    "            \n",
    "            if isinstance(outputs, list):\n",
    "                test_loss += sum([criterion(output, masks).item() for output in outputs]) / len(outputs)\n",
    "                test_dice += sum([dice_coefficient(output, masks).item() for output in outputs]) / len(outputs)\n",
    "                output_for_vis = outputs[-1]\n",
    "            else:\n",
    "                test_loss += criterion(outputs, masks).item()\n",
    "                test_dice += dice_coefficient(outputs, masks).item()\n",
    "                output_for_vis = outputs\n",
    "\n",
    "            visualize_results(\n",
    "                images[0].cpu(),\n",
    "                masks[0].cpu(),\n",
    "                output_for_vis[0].detach().cpu(),\n",
    "                questions[0],\n",
    "                os.path.join(test_folder, f\"test_sample_{i}.png\")\n",
    "            )\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_test_dice = test_dice / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Dice: {avg_test_dice:.4f}\")\n",
    "\n",
    "def main(csv_file):\n",
    "    output_folder = \"segmentation_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Separate transforms for images and masks\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=True)\n",
    "    val_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "    test_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "    train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = Subset(train_dataset, train_indices)\n",
    "    val_data = Subset(val_dataset, val_indices)\n",
    "    test_data = Subset(test_dataset, test_indices)\n",
    "\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LLMSupervisedSAM(num_classes=1, input_channels=3, deep_supervision=False)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    num_epochs = 100\n",
    "    train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device, output_folder)\n",
    "\n",
    "    print(\"Training and evaluation completed. Results saved in the 'segmentation_results' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"../retinal_segmentation/segmentation/final_data_for_segmentation/final_dataset.csv\"\n",
    "    main(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tried increasing the model complexity here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50:  44%|████▍     | 133/304 [13:05<16:49,  5.91s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_182145/1824209623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/chethanat/retinal_segmentation/segmentation/final_data_for_segmentation/final_dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_182145/1824209623.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# Reduced number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training and evaluation completed. Results saved in the 'segmentation_results' folder.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_182145/1824209623.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder)\u001b[0m\n\u001b[1;32m    269\u001b[0m                     \u001b[0moutput_for_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Import the UNet++ model\n",
    "from unetplus import NestedUNet\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextmanager\n",
    "def suppress_output():\n",
    "    \"\"\"Suppress all output except tqdm progress bars.\"\"\"\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "class CataractDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_transform=None, mask_transform=None, augment=False):\n",
    "        self.data = []\n",
    "        self.questions = set()\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.data.append(row)\n",
    "                self.questions.add(row['Questions'])\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['Image_Paths']).convert('RGB')\n",
    "        mask = Image.open(item['Mask_Paths']).convert('L')\n",
    "        question = item['Questions']\n",
    "        label = item['Labels']\n",
    "\n",
    "        if self.augment:\n",
    "            image, mask = self.apply_augmentation(image, mask)\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, question, label\n",
    "\n",
    "    def apply_augmentation(self, image, mask):\n",
    "        # Simpler augmentations\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        \n",
    "        angle = random.uniform(-10, 10)\n",
    "        image = TF.rotate(image, angle)\n",
    "        mask = TF.rotate(mask, angle)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_channels=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        # Reduce model complexity by reducing depth or the number of filters\n",
    "        self.unetpp = NestedUNet(num_classes=num_classes, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unetpp(x)\n",
    "\n",
    "class LLMPromptEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 1024)\n",
    "\n",
    "    def forward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upconv_block(in_channels, 256),\n",
    "            self.upconv_block(256, 128),\n",
    "            self.upconv_block(128, 64),\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class LLMSupervisedSAM(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_channels=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(num_classes=num_classes, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.prompt_encoder = LLMPromptEncoder()\n",
    "        self.mask_decoder = MaskDecoder(in_channels=num_classes + 1024)\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "    def forward(self, image, questions):\n",
    "        image_features = self.image_encoder(image)\n",
    "        \n",
    "        batch_size = image.size(0)\n",
    "        device = image.device\n",
    "        \n",
    "        sliced_questions = questions[:batch_size]\n",
    "        prompt_features = self.prompt_encoder(sliced_questions)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            outputs = []\n",
    "            for feature in image_features:\n",
    "                resized_prompt = F.interpolate(prompt_features.view(batch_size, 1024, 1, 1),\n",
    "                                               size=feature.shape[2:], \n",
    "                                               mode='bilinear', \n",
    "                                               align_corners=False)\n",
    "                combined_features = torch.cat([feature, resized_prompt], dim=1)\n",
    "                mask = self.mask_decoder(combined_features)\n",
    "                outputs.append(mask)\n",
    "            return outputs\n",
    "        else:\n",
    "            prompt_features = prompt_features.view(batch_size, 1024, 1, 1).expand(-1, -1, image_features.shape[2], image_features.shape[3])\n",
    "            combined_features = torch.cat([image_features, prompt_features], dim=1)\n",
    "            mask = self.mask_decoder(combined_features)\n",
    "            return mask\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def visualize_results(image, mask, prediction, question, output_path):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image = denormalize(image.clone(), mean, std)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.imshow(mask.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.imshow(prediction.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Question: {question}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = self.bce_loss(logits, targets)\n",
    "        dice = self.dice_loss(torch.sigmoid(logits), targets)\n",
    "        return self.alpha * bce + self.beta * dice\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (logits * targets).sum()\n",
    "        return 1 - ((2. * intersection + self.smooth) / (logits.sum() + targets.sum() + self.smooth))\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder):\n",
    "    train_folder = os.path.join(output_folder, \"train\")\n",
    "    val_folder = os.path.join(output_folder, \"validation\")\n",
    "    test_folder = os.path.join(output_folder, \"test\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    best_val_dice = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_dice = 0\n",
    "        for i, (images, masks, questions, _) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            questions = questions[:batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images, questions)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                    dice = sum([dice_coefficient(torch.sigmoid(output), masks) for output in outputs]) / len(outputs)\n",
    "                    output_for_vis = outputs[-1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    dice = dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "                    output_for_vis = outputs\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "            if i in [0, len(train_loader) // 2, len(train_loader) - 1]:\n",
    "                visualize_results(\n",
    "                    images[0].cpu(),\n",
    "                    masks[0].cpu(),\n",
    "                    torch.sigmoid(output_for_vis[0]).detach().cpu(),\n",
    "                    questions[0], \n",
    "                    os.path.join(train_folder, f\"epoch_{epoch+1}_batch_{i}.png\")\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_dice = total_dice / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_dice = 0\n",
    "        for i, (images, masks, questions, _) in enumerate(tqdm(val_loader, desc=\"Validation\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            questions = questions[:batch_size]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, questions)\n",
    "\n",
    "                if isinstance(outputs, list):\n",
    "                    val_loss += sum([criterion(output, masks).item() for output in outputs]) / len(outputs)\n",
    "                    val_dice += sum([dice_coefficient(torch.sigmoid(output), masks).item() for output in outputs]) / len(outputs)\n",
    "                    output_for_vis = outputs[-1]\n",
    "                else:\n",
    "                    val_loss += criterion(outputs, masks).item()\n",
    "                    val_dice += dice_coefficient(torch.sigmoid(outputs), masks).item()\n",
    "                    output_for_vis = outputs\n",
    "\n",
    "            if i in [0, len(val_loader) // 2, len(val_loader) - 1]:\n",
    "                visualize_results(\n",
    "                    images[0].cpu(),\n",
    "                    masks[0].cpu(),\n",
    "                    torch.sigmoid(output_for_vis[0]).detach().cpu(),\n",
    "                    questions[0],\n",
    "                    os.path.join(val_folder, f\"epoch_{epoch+1}_batch_{i}.png\")\n",
    "                )\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = val_dice / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Dice: {avg_val_dice:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            torch.save(model.state_dict(), os.path.join(output_folder, \"best_model.pth\"))\n",
    "\n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(torch.load(os.path.join(output_folder, \"best_model.pth\")))\n",
    "\n",
    "    # Test set evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_dice = 0\n",
    "    for i, (images, masks, questions, _) in enumerate(tqdm(test_loader, desc=\"Testing\", unit=\"batch\")):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        questions = questions[:batch_size]\n",
    "        outputs = model(images, questions)\n",
    "\n",
    "        if isinstance(outputs, list):\n",
    "            test_loss += sum([criterion(output, masks).item() for output in outputs]) / len(outputs)\n",
    "            test_dice += sum([dice_coefficient(output, masks).item() for output in outputs]) / len(outputs)\n",
    "            output_for_vis = outputs[-1]\n",
    "        else:\n",
    "            test_loss += criterion(outputs, masks).item()\n",
    "            test_dice += dice_coefficient(outputs, masks).item()\n",
    "            output_for_vis = outputs\n",
    "\n",
    "        visualize_results(\n",
    "            images[0].cpu(),\n",
    "            masks[0].cpu(),\n",
    "            output_for_vis[0].detach().cpu(),\n",
    "            questions[0],\n",
    "            os.path.join(test_folder, f\"test_sample_{i}.png\")\n",
    "        )\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_test_dice = test_dice / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Dice: {avg_test_dice:.4f}\")\n",
    "\n",
    "def main(csv_file):\n",
    "    output_folder = \"segmentation_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Separate transforms for images and masks\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=True)\n",
    "    val_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "    test_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "    train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = Subset(train_dataset, train_indices)\n",
    "    val_data = Subset(val_dataset, val_indices)\n",
    "    test_data = Subset(test_dataset, test_indices)\n",
    "\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LLMSupervisedSAM(num_classes=1, input_channels=3, deep_supervision=True)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = CombinedLoss(alpha=0.7, beta=0.3)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    num_epochs = 50  # Reduced number of epochs\n",
    "    train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder)\n",
    "\n",
    "    print(\"Training and evaluation completed. Results saved in the 'segmentation_results' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"../retinal_segmentation/segmentation/final_data_for_segmentation/final_dataset.csv\"\n",
    "    main(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added early stopping, loss fuctions, data agumentation and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/500: 100%|██████████| 152/152 [00:39<00:00,  3.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.5975, Dice: 0.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5864, Validation Dice: 0.2809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/500: 100%|██████████| 152/152 [00:39<00:00,  3.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500], Loss: 0.5655, Dice: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5634, Validation Dice: 0.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/500: 100%|██████████| 152/152 [00:39<00:00,  3.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500], Loss: 0.5344, Dice: 0.3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4884, Validation Dice: 0.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500], Loss: 0.4967, Dice: 0.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5076, Validation Dice: 0.4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/500: 100%|██████████| 152/152 [00:39<00:00,  3.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/500], Loss: 0.4620, Dice: 0.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4464, Validation Dice: 0.4655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/500: 100%|██████████| 152/152 [00:39<00:00,  3.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/500], Loss: 0.4251, Dice: 0.5038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4694, Validation Dice: 0.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/500: 100%|██████████| 152/152 [00:40<00:00,  3.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/500], Loss: 0.3952, Dice: 0.5307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3933, Validation Dice: 0.5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/500: 100%|██████████| 152/152 [00:40<00:00,  3.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/500], Loss: 0.3691, Dice: 0.5558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3693, Validation Dice: 0.5350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/500: 100%|██████████| 152/152 [00:40<00:00,  3.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/500], Loss: 0.3511, Dice: 0.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3676, Validation Dice: 0.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.3411, Dice: 0.5860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3619, Validation Dice: 0.5722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/500: 100%|██████████| 152/152 [00:39<00:00,  3.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/500], Loss: 0.3337, Dice: 0.5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3457, Validation Dice: 0.5928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/500], Loss: 0.3268, Dice: 0.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3331, Validation Dice: 0.5839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/500], Loss: 0.3228, Dice: 0.6055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3341, Validation Dice: 0.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/500: 100%|██████████| 152/152 [00:41<00:00,  3.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/500], Loss: 0.3196, Dice: 0.6089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3270, Validation Dice: 0.6070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/500: 100%|██████████| 152/152 [00:40<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/500], Loss: 0.3155, Dice: 0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3213, Validation Dice: 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/500: 100%|██████████| 152/152 [00:40<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/500], Loss: 0.3149, Dice: 0.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  5.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3235, Validation Dice: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/500: 100%|██████████| 152/152 [00:40<00:00,  3.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/500], Loss: 0.3128, Dice: 0.6153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3248, Validation Dice: 0.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/500: 100%|██████████| 152/152 [00:40<00:00,  3.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/500], Loss: 0.3105, Dice: 0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3213, Validation Dice: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/500: 100%|██████████| 152/152 [00:40<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/500], Loss: 0.3095, Dice: 0.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  5.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3215, Validation Dice: 0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/500: 100%|██████████| 152/152 [00:40<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500], Loss: 0.3075, Dice: 0.6216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3194, Validation Dice: 0.5935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/500: 100%|██████████| 152/152 [00:40<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/500], Loss: 0.3055, Dice: 0.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3307, Validation Dice: 0.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/500: 100%|██████████| 152/152 [00:40<00:00,  3.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/500], Loss: 0.3069, Dice: 0.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3299, Validation Dice: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/500], Loss: 0.3037, Dice: 0.6244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3248, Validation Dice: 0.6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/500: 100%|██████████| 152/152 [00:40<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/500], Loss: 0.3046, Dice: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3277, Validation Dice: 0.6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/500: 100%|██████████| 152/152 [00:39<00:00,  3.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/500], Loss: 0.3041, Dice: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3272, Validation Dice: 0.5929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/500], Loss: 0.3019, Dice: 0.6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3338, Validation Dice: 0.5776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/500], Loss: 0.2997, Dice: 0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3240, Validation Dice: 0.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/500: 100%|██████████| 152/152 [00:40<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/500], Loss: 0.2985, Dice: 0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3174, Validation Dice: 0.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/500: 100%|██████████| 152/152 [00:39<00:00,  3.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/500], Loss: 0.2979, Dice: 0.6288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3212, Validation Dice: 0.6041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/500: 100%|██████████| 152/152 [00:40<00:00,  3.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/500], Loss: 0.2974, Dice: 0.6280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3189, Validation Dice: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31/500: 100%|██████████| 152/152 [00:40<00:00,  3.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/500], Loss: 0.2960, Dice: 0.6303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  5.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3203, Validation Dice: 0.6013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32/500: 100%|██████████| 152/152 [00:40<00:00,  3.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/500], Loss: 0.2947, Dice: 0.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3194, Validation Dice: 0.6034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33/500: 100%|██████████| 152/152 [00:40<00:00,  3.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/500], Loss: 0.2958, Dice: 0.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:06<00:00,  6.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3201, Validation Dice: 0.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34/500: 100%|██████████| 152/152 [00:40<00:00,  3.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/500], Loss: 0.2966, Dice: 0.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3282, Validation Dice: 0.5858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35/500: 100%|██████████| 152/152 [00:40<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/500], Loss: 0.2950, Dice: 0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:07<00:00,  5.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3231, Validation Dice: 0.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36/500: 100%|██████████| 152/152 [00:39<00:00,  3.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/500], Loss: 0.2932, Dice: 0.6323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3246, Validation Dice: 0.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37/500: 100%|██████████| 152/152 [00:40<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/500], Loss: 0.2939, Dice: 0.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 38/38 [00:05<00:00,  6.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3185, Validation Dice: 0.6029\n",
      "Early stopping triggered after 37 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 48/48 [00:07<00:00,  6.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3275, Test Dice: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.transforms import RandomAffine, ColorJitter\n",
    "\n",
    "# Import the NestedUNet model\n",
    "from unetplus import NestedUNet\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextmanager\n",
    "def suppress_output():\n",
    "    \"\"\"Suppress all output except tqdm progress bars.\"\"\"\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "class CataractDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_transform=None, mask_transform=None, augment=False):\n",
    "        self.data = []\n",
    "        self.questions = set()\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.data.append(row)\n",
    "                self.questions.add(row['Questions'])\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['Image_Paths']).convert('RGB')\n",
    "        mask = Image.open(item['Mask_Paths']).convert('L')\n",
    "        question = item['Questions']\n",
    "        label = item['Labels']\n",
    "\n",
    "        if self.augment:\n",
    "            image, mask = self.apply_augmentation(image, mask)\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, question, label\n",
    "\n",
    "    def apply_augmentation(self, image, mask):\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        \n",
    "        angle = random.uniform(-15, 15)\n",
    "        image = TF.rotate(image, angle)\n",
    "        mask = TF.rotate(mask, angle)\n",
    "        \n",
    "        affine_params = RandomAffine.get_params(degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=image.size)\n",
    "        image = TF.affine(image, *affine_params)\n",
    "        mask = TF.affine(mask, *affine_params)\n",
    "        \n",
    "        color_jitter = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "class LLMSupervisedSAM(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_channels=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        self.image_encoder = NestedUNet(num_classes=256, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.prompt_encoder = nn.Linear(768, 256)\n",
    "        self.final_conv = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        # Add batch normalization and dropout\n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, image, prompt_embedding):\n",
    "        image_features = self.image_encoder(image)\n",
    "        prompt_features = self.prompt_encoder(prompt_embedding)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            outputs = []\n",
    "            for feature in image_features:\n",
    "                combined_features = feature + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(feature)\n",
    "                combined_features = self.bn(combined_features)\n",
    "                combined_features = self.dropout(combined_features)\n",
    "                output = self.final_conv(combined_features)\n",
    "                outputs.append(output)\n",
    "            return outputs\n",
    "        else:\n",
    "            combined_features = image_features + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(image_features)\n",
    "            combined_features = self.bn(combined_features)\n",
    "            combined_features = self.dropout(combined_features)\n",
    "            output = self.final_conv(combined_features)\n",
    "            return output\n",
    "        \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        focal = self.focal_loss(logits, targets)\n",
    "        dice = self.dice_loss(torch.sigmoid(logits), targets)\n",
    "        return self.alpha * focal + self.beta * dice\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (logits * targets).sum()\n",
    "        return 1 - ((2. * intersection + self.smooth) / (logits.sum() + targets.sum() + self.smooth))\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder):\n",
    "    train_folder = os.path.join(output_folder, \"train\")\n",
    "    val_folder = os.path.join(output_folder, \"validation\")\n",
    "    test_folder = os.path.join(output_folder, \"test\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    best_val_dice = 0\n",
    "    patience = 20  # Increased patience\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_dice = 0\n",
    "        for i, (images, masks, questions, _) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            prompt_embeddings = torch.randn(batch_size, 768).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images, prompt_embeddings)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                    dice = sum([dice_coefficient(torch.sigmoid(output), masks) for output in outputs]) / len(outputs)\n",
    "                    output_for_vis = outputs[-1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    dice = dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "                    output_for_vis = outputs\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                visualize_results(\n",
    "                    images[0].cpu(),\n",
    "                    masks[0].cpu(),\n",
    "                    torch.sigmoid(output_for_vis[0]).detach().cpu(),\n",
    "                    questions[0], \n",
    "                    os.path.join(train_folder, f\"epoch_{epoch+1}_batch_{i}.png\")\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_dice = total_dice / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "\n",
    "        val_loss, val_dice = evaluate(model, val_loader, criterion, device, val_folder, epoch)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            torch.save(model.state_dict(), os.path.join(output_folder, \"best_model.pth\"))\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(os.path.join(output_folder, \"best_model.pth\")))\n",
    "    test_loss, test_dice = evaluate(model, test_loader, criterion, device, test_folder, \"test\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Dice: {test_dice:.4f}\")\n",
    "\n",
    "# Make sure to define the evaluate function if it's not already defined\n",
    "def evaluate(model, data_loader, criterion, device, output_folder, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, questions, _) in enumerate(tqdm(data_loader, desc=\"Evaluation\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            prompt_embeddings = torch.randn(batch_size, 768).to(device)\n",
    "            \n",
    "            outputs = model(images, prompt_embeddings)\n",
    "\n",
    "            if isinstance(outputs, list):\n",
    "                loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                dice = sum([dice_coefficient(torch.sigmoid(output), masks) for output in outputs]) / len(outputs)\n",
    "                output_for_vis = outputs[-1]\n",
    "            else:\n",
    "                loss = criterion(outputs, masks)\n",
    "                dice = dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "                output_for_vis = outputs\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                visualize_results(\n",
    "                    images[0].cpu(),\n",
    "                    masks[0].cpu(),\n",
    "                    torch.sigmoid(output_for_vis[0]).detach().cpu(),\n",
    "                    questions[0],\n",
    "                    os.path.join(output_folder, f\"epoch_{epoch}_batch_{i}.png\")\n",
    "                )\n",
    "\n",
    "    return total_loss / len(data_loader), total_dice / len(data_loader)\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "def visualize_results(image, mask, prediction, question, output_path):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image = denormalize(image.clone(), mean, std)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.imshow(mask.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "    plt.imshow(prediction.squeeze(), alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Question: {question}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def main(csv_file):\n",
    "    output_folder = \"segmentation_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=True)\n",
    "    val_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "    test_dataset = CataractDataset(csv_file, image_transform=image_transform, mask_transform=mask_transform, augment=False)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "    train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = Subset(train_dataset, train_indices)\n",
    "    val_data = Subset(val_dataset, val_indices)\n",
    "    test_data = Subset(test_dataset, test_indices)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "\n",
    "    model = LLMSupervisedSAM(num_classes=1, input_channels=3, deep_supervision=True)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = CombinedLoss(alpha=0.3, beta=0.7)  # Adjusted weights\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Reduced learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  # Changed scheduler\n",
    "\n",
    "    num_epochs = 500  # Increased number of epochs\n",
    "    train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"../retinal_segmentation/segmentation/final_data_for_segmentation/final_dataset.csv\"\n",
    "    main(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## channels modified "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Scheduling:\n",
    "Implement a learning rate scheduler that reduces the learning rate when performance plateaus.\n",
    "Data Augmentation:\n",
    "Implement more aggressive data augmentation techniques to increase the diversity of your training data.\n",
    "Initially, image was 3 channel and mask was 1 channel(since it was not rgb form)- modify it to 3 channels \n",
    "try using layer normalization instead of batch normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Import the NestedUNet model\n",
    "from unetplus import NestedUNet\n",
    "\n",
    "class CataractDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, augment=False):\n",
    "        self.data = []\n",
    "        self.questions = set()\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.data.append(row)\n",
    "                self.questions.add(row['Questions'])\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Check if the image and mask paths exist\n",
    "        if not os.path.exists(item['Image_Paths']) or not os.path.exists(item['Mask_Paths']):\n",
    "            raise FileNotFoundError(f\"Image or mask not found: {item['Image_Paths']} or {item['Mask_Paths']}\")\n",
    "\n",
    "        try:\n",
    "            image = Image.open(item['Image_Paths']).convert('RGB')\n",
    "            mask = Image.open(item['Mask_Paths']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error opening image or mask: {e}\")\n",
    "\n",
    "        question = item['Questions']\n",
    "        label = item['Labels']\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        # Convert mask to float and normalize to [0, 1]\n",
    "        mask = mask.float() / 255.0\n",
    "\n",
    "        # Ensure mask is in the correct format (B, C, H, W)\n",
    "        mask = mask.permute(2, 0, 1)  # Change from (H, W, C) to (C, H, W)\n",
    "\n",
    "        return image, mask, question, label\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Flip(p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "class LLMSupervisedSAM(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=3, deep_supervision=True):\n",
    "        super().__init__()\n",
    "        self.image_encoder = NestedUNet(num_classes=256, input_channels=input_channels, deep_supervision=deep_supervision)\n",
    "        self.prompt_encoder = nn.Linear(768, 256)\n",
    "        self.final_convs = nn.ModuleList([nn.Conv2d(256, num_classes, kernel_size=1) for _ in range(5)])\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        self.ln = nn.LayerNorm([256, 256, 256])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, image, prompt_embedding):\n",
    "        image_features = self.image_encoder(image)\n",
    "        prompt_features = self.prompt_encoder(prompt_embedding)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            outputs = []\n",
    "            for i, feature in enumerate(image_features):\n",
    "                combined_features = feature + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(feature)\n",
    "                combined_features = self.ln(combined_features)\n",
    "                combined_features = self.dropout(combined_features)\n",
    "                output = self.final_convs[i](combined_features)\n",
    "                output = torch.sigmoid(output)  # Apply sigmoid to ensure output is in [0, 1] range\n",
    "                outputs.append(output)\n",
    "            return outputs\n",
    "        else:\n",
    "            combined_features = image_features[-1] + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(image_features[-1])\n",
    "            combined_features = self.ln(combined_features)\n",
    "            combined_features = self.dropout(combined_features)\n",
    "            output = self.final_convs[-1](combined_features)\n",
    "            output = torch.sigmoid(output)  # Apply sigmoid to ensure output is in [0, 1] range\n",
    "            return output\n",
    "\n",
    "    def get_attention_map(self, image, prompt_embedding):\n",
    "        image_features = self.image_encoder(image)\n",
    "        prompt_features = self.prompt_encoder(prompt_embedding)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            attention_maps = []\n",
    "            for i, feature in enumerate(image_features):\n",
    "                combined_features = feature + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(feature)\n",
    "                attention_map = F.softmax(combined_features.sum(dim=1), dim=-1)\n",
    "                attention_maps.append(attention_map)\n",
    "            return attention_maps\n",
    "        else:\n",
    "            combined_features = image_features[-1] + prompt_features.unsqueeze(2).unsqueeze(3).expand_as(image_features[-1])\n",
    "            attention_map = F.softmax(combined_features.sum(dim=1), dim=-1)\n",
    "            return attention_map\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        focal = self.focal_loss(logits, targets)\n",
    "        dice = self.dice_loss(torch.sigmoid(logits), targets)\n",
    "        return self.alpha * focal + self.beta * dice\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder):\n",
    "    scaler = GradScaler()\n",
    "    best_val_dice = 0\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_dice = 0\n",
    "        for i, (images, masks, questions, _) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            prompt_embeddings = torch.randn(batch_size, 768).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images, prompt_embeddings)\n",
    "                \n",
    "                if isinstance(outputs, list):\n",
    "                    loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                    dice = sum([dice_coefficient(output, masks) for output in outputs]) / len(outputs)\n",
    "                else:\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    dice = dice_coefficient(outputs, masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_dice = total_dice / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_dice = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            torch.save(model.state_dict(), os.path.join(output_folder, \"best_model.pth\"))\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # Load the best model for final evaluation\n",
    "    model.load_state_dict(torch.load(os.path.join(output_folder, \"best_model.pth\")))\n",
    "    test_loss, test_dice = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Dice: {test_dice:.4f}\")\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, _, _) in enumerate(tqdm(data_loader, desc=\"Evaluation\", unit=\"batch\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            prompt_embeddings = torch.randn(batch_size, 768).to(device)\n",
    "            \n",
    "            outputs = model(images, prompt_embeddings)\n",
    "\n",
    "            if isinstance(outputs, list):\n",
    "                loss = sum([criterion(output, masks) for output in outputs]) / len(outputs)\n",
    "                dice = sum([dice_coefficient(torch.sigmoid(output), masks) for output in outputs]) / len(outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, masks)\n",
    "                dice = dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "\n",
    "    return total_loss / len(data_loader), total_dice / len(data_loader)\n",
    "\n",
    "def main(csv_file):\n",
    "    output_folder = \"segmentation_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    train_transform = get_transforms(train=True)\n",
    "    val_transform = get_transforms(train=False)\n",
    "\n",
    "    train_dataset = CataractDataset(csv_file, transform=train_transform, augment=True)\n",
    "    val_dataset = CataractDataset(csv_file, transform=val_transform, augment=False)\n",
    "    test_dataset = CataractDataset(csv_file, transform=val_transform, augment=False)\n",
    "\n",
    "    train_indices, test_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
    "    train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = Subset(train_dataset, train_indices)\n",
    "    val_data = Subset(val_dataset, val_indices)\n",
    "    test_data = Subset(test_dataset, test_indices)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LLMSupervisedSAM(num_classes=3, input_channels=3, deep_supervision=True)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = CombinedLoss(alpha=0.5, beta=0.5)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    num_epochs = 500\n",
    "    train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"../retinal_segmentation/segmentation/final_data_for_segmentation/final_dataset.csv\"\n",
    "    main(csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
